{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5135b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbc17e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_nonlinear = pd.read_csv('matching_table/mono_match_nonlinear.csv')['csv'].values\n",
    "mono_nonlinear = np.unique(mono_nonlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8152e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_linear = pd.read_csv('matching_table/mono_match.csv')['csv'].values\n",
    "mono_linear = np.unique(mono_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245765cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mono = np.unique(np.concatenate([mono_linear, mono_nonlinear]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe3dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa26671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer_trainer(alg):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
    "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
    "    elif alg == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
    "    elif alg == 'WPC':\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
    "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
    "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
    "    \n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725b7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(files, alg='WLV'):\n",
    "    \"\"\"\n",
    "    Takes the files and trains the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
    "    tokenizer.train(files, trainer) # training the tokenzier\n",
    "    tokenizer.save(\"./tokenizer-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ccfe686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in all_mono:\n",
    "    \n",
    "#     with open(os.path.join('matching_table/mono_list/') + i + '.txt', 'x') as f:\n",
    "#         f.write(i)\n",
    "#         f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f83874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = 'matching_table/mono_list/'\n",
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('matching_table/mono_list/').glob('**/*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cf84c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False\n",
    ")\n",
    "# and train\n",
    "tokenizer.train(files=paths, vocab_size=32, min_frequency=1,\n",
    "                limit_alphabet=32, wordpieces_prefix='##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44210e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 34,\n",
       " 'h': 31,\n",
       " '##h': 58,\n",
       " 'I': 18,\n",
       " '##T': 64,\n",
       " '-': 5,\n",
       " 'G': 16,\n",
       " '##H': 55,\n",
       " '##o': 57,\n",
       " '[UNK]': 1,\n",
       " 'D': 13,\n",
       " 'M': 20,\n",
       " 'P': 23,\n",
       " '4': 9,\n",
       " 'A': 10,\n",
       " '##b': 59,\n",
       " '##3': 43,\n",
       " 'B': 11,\n",
       " '##O': 50,\n",
       " 'R': 24,\n",
       " 'l': 32,\n",
       " '[MASK]': 4,\n",
       " '##a': 45,\n",
       " '##M': 65,\n",
       " '##n': 54,\n",
       " '##D': 60,\n",
       " 'U': 27,\n",
       " '1': 6,\n",
       " '[SEP]': 3,\n",
       " '##4': 51,\n",
       " '##B': 63,\n",
       " '##F': 62,\n",
       " '##2': 56,\n",
       " '##p': 49,\n",
       " '##1': 42,\n",
       " '##P': 39,\n",
       " '##l': 46,\n",
       " '##A': 40,\n",
       " '3': 8,\n",
       " 'a': 28,\n",
       " '##u': 47,\n",
       " '[PAD]': 0,\n",
       " 'u': 36,\n",
       " '##N': 41,\n",
       " 'O': 22,\n",
       " 'H': 17,\n",
       " '##c': 48,\n",
       " '##R': 61,\n",
       " 'S': 25,\n",
       " '2': 7,\n",
       " 'F': 15,\n",
       " 'L': 19,\n",
       " 'b': 29,\n",
       " '##L': 44,\n",
       " '##C': 38,\n",
       " 'T': 26,\n",
       " 'C': 12,\n",
       " '##I': 52,\n",
       " '[CLS]': 2,\n",
       " '##E': 66,\n",
       " 'p': 35,\n",
       " '##S': 53,\n",
       " '##U': 37,\n",
       " 'E': 14,\n",
       " 'n': 33,\n",
       " 'N': 21,\n",
       " 'c': 30}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f62951a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A-D-FUCP', 'A-D-FUCPNAC', 'A-D-GALF', 'A-D-GALNAC', 'A-D-GALP',\n",
       "       'A-D-GALPA', 'A-D-GALPA2SO3', 'A-D-GALPAN', 'A-D-GALPASO3',\n",
       "       'A-D-GALPNAC', 'A-D-GLC', 'A-D-GLCP', 'A-D-GLCP1OME', 'A-D-GLCPA',\n",
       "       'A-D-GLCPN', 'A-D-GLCPN1PO4', 'A-D-GLCPNAC', 'A-D-KDOP',\n",
       "       'A-D-MANP', 'A-D-MANP1SET', 'A-D-MANP2AC', 'A-D-MANP2AC1SET',\n",
       "       'A-D-MANP3AC', 'A-D-MANP4AC', 'A-D-MANP6AC', 'A-D-QUIP4N',\n",
       "       'A-D-RHAP', 'A-D-RHAP4NAC', 'A-L-ARAF', 'A-L-ARAP', 'A-L-FUCP',\n",
       "       'A-L-FUCP2SO3', 'A-L-FUCP2SO33SO34SO3', 'A-L-FUCP2SO34SO3',\n",
       "       'A-L-FUCP3SO34SO3', 'A-L-FUCPNAC', 'A-L-GALP', 'A-L-GULPA',\n",
       "       'A-L-RHAP', 'B-D-FUCPNAC', 'B-D-GALF', 'B-D-GALF2AC',\n",
       "       'B-D-GALFOAC', 'B-D-GALP', 'B-D-GALP1OME', 'B-D-GALPA',\n",
       "       'B-D-GALPNAC', 'B-D-GLC1OME', 'B-D-GLCP', 'B-D-GLCP1OME',\n",
       "       'B-D-GLCP2AC', 'B-D-GLCPA', 'B-D-GLCPAN', 'B-D-GLCPN',\n",
       "       'B-D-GLCPN4PO4', 'B-D-GLCPNAC', 'B-D-GLCPNAC6SO3', 'B-D-KDOP',\n",
       "       'B-D-MANP', 'B-D-MANP2AC', 'B-D-MANPA1NAC3NAC', 'B-D-MANPNAC',\n",
       "       'B-D-QUIP', 'B-D-QUIP3NAC', 'B-D-QUIP4N', 'B-D-QUIP4NAC',\n",
       "       'B-D-RHAP', 'B-D-RHAP4NAC', 'B-D-RIBF', 'B-D-XYLP', 'B-L-ARAP',\n",
       "       'B-L-FUCP', 'B-L-GALP', 'B-L-GALP1OME', 'B-L-RHAP',\n",
       "       'B-L-RHAPNAC3NAC', 'B-L-XYLP', 'D-A-D-HEPP', 'D-GLCPA', 'D-GLCPN',\n",
       "       'DELTA-GLCPAN', 'KDN', 'KDOP', 'L-A-D-HEPP', 'L-gro-a-D-manHepp',\n",
       "       'Monosaccharid', 'ONONITOL', 'Tyr', 'a-D-Fucp', 'a-D-Galf',\n",
       "       'a-D-Galp', 'a-D-GalpA', 'a-D-GalpANAc', 'a-D-GalpNAc', 'a-D-Glcp',\n",
       "       'a-D-GlcpA', 'a-D-GlcpNAc', 'a-D-Kdop', 'a-D-Manp', 'a-D-ManpNAc',\n",
       "       'a-D-Neup5Ac', 'a-D-Rhap', 'a-D-Xylf', 'a-D-Xylp',\n",
       "       'a-L-6-deoxy-Talp', 'a-L-Fucp', 'a-L-FucpNAc', 'a-L-IdopA',\n",
       "       'a-L-Rhap', 'a-L-Rhap2Ac', 'a-L-Rhap3Me', 'b-D-Fucp3NAc',\n",
       "       'b-D-Galf', 'b-D-Galf2Ac', 'b-D-Galp', 'b-D-GalpA', 'b-D-GalpNAc',\n",
       "       'b-D-GalpNAc3Ac', 'b-D-Glcp', 'b-D-GlcpA', 'b-D-GlcpN',\n",
       "       'b-D-GlcpNAc', 'b-D-Manp', 'b-D-ManpNAc', 'b-D-Rhap', 'b-D-Ribf',\n",
       "       'b-L-Rhap', 'b-L-RhapNAc3NAc'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mono"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
